{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#DenseNet121 classification\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths\n",
    "train_val_dir = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\HyperKvasir Classification\"\n",
    "\n",
    "# Data preparation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define classes\n",
    "classes = ['Mayo 0','Mayo 1','Mayo 2','Mayo 3']\n",
    "\n",
    "# Load data with correct class_mode='sparse'\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# Print class indices and expected number of batches\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "print(\"Expected number of batches:\", len(train_generator))\n",
    "\n",
    "# Load MobileNet model without the top layers\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(generator, model, num_augmentations=5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_batches = num_samples // generator.batch_size + (1 if num_samples % generator.batch_size else 0)\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        generator.reset()\n",
    "        for _ in range(num_batches):\n",
    "            inputs_batch, labels_batch = next(generator)\n",
    "            features_batch = model.predict(inputs_batch)\n",
    "            features.append(features_batch)\n",
    "            labels.append(labels_batch)\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(train_generator, model)\n",
    "\n",
    "# Troubleshooting: Check unique classes and their distribution\n",
    "print(\"Unique classes:\", np.unique(train_labels))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initial train-test split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Reshape features for SMOTE (only for training set)\n",
    "n_samples, x, y, z = x_train.shape\n",
    "x_train_reshaped = x_train.reshape((n_samples, x * y * z))\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    # Reshape back to original feature shape\n",
    "    x_train_smote = x_train_smote.reshape((-1, x, y, z))\n",
    "else:\n",
    "    print(\"Skipping SMOTE due to insufficient classes\")\n",
    "    x_train_smote = x_train\n",
    "    y_train_smote = y_train\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"x_train shape:\", x_train_smote.shape)\n",
    "print(\"y_train shape:\", y_train_smote.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create MobileNet model\n",
    "def create_mobilenet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(4, activation='softmax')(x)  # 4 output neurons for 4 categories\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Get the shape of the extracted features\n",
    "input_shape = train_features.shape[1:]\n",
    "\n",
    "model = create_mobilenet_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\densenet_legit_compleltly_smote_model_900_classification_final_2.keras'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model.fit(\n",
    "    x_train_smote, y_train_smote,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=900,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "final_model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\densenet_legit_compleltly_smote_model_900_classification_final_2.keras'\n",
    "model.save(final_model_save_path)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Predicted class labels (argmax gives the class with highest probability)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Display visual confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# Calculate F1 score, precision, and recall (sensitivity) for each class\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum() - (cm[class_label].sum() + cm[:, class_label].sum() - cm[class_label, class_label])\n",
    "    fp = cm[:, class_label].sum() - cm[class_label, class_label]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity = [specificity_score(y_test, y_pred, i) for i in range(len(classes))]\n",
    "\n",
    "# Print results for each class\n",
    "print(\"\\nMetrics for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"F1 Score: {f1[i]:.4f}\")\n",
    "    print(f\"Precision: {precision[i]:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall[i]:.4f}\")\n",
    "    print(f\"Specificity: {specificity[i]:.4f}\")\n",
    "\n",
    "# Calculate Cohen's Kappa coefficient\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"\\nCohen's Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InceptionV3 0-1 classification\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths\n",
    "train_val_dir = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\0-1 classification\"\n",
    "\n",
    "# Data preparation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define classes\n",
    "classes = ['Mayo 0','Mayo 1']\n",
    "\n",
    "# Load data with correct class_mode='sparse'\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# Print class indices and expected number of batches\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "print(\"Expected number of batches:\", len(train_generator))\n",
    "\n",
    "# Load MobileNet model without the top layers\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(generator, model, num_augmentations=5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_batches = num_samples // generator.batch_size + (1 if num_samples % generator.batch_size else 0)\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        generator.reset()\n",
    "        for _ in range(num_batches):\n",
    "            inputs_batch, labels_batch = next(generator)\n",
    "            features_batch = model.predict(inputs_batch)\n",
    "            features.append(features_batch)\n",
    "            labels.append(labels_batch)\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(train_generator, model)\n",
    "\n",
    "# Troubleshooting: Check unique classes and their distribution\n",
    "print(\"Unique classes:\", np.unique(train_labels))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initial train-test split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Reshape features for SMOTE (only for training set)\n",
    "n_samples, x, y, z = x_train.shape\n",
    "x_train_reshaped = x_train.reshape((n_samples, x * y * z))\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    # Reshape back to original feature shape\n",
    "    x_train_smote = x_train_smote.reshape((-1, x, y, z))\n",
    "else:\n",
    "    print(\"Skipping SMOTE due to insufficient classes\")\n",
    "    x_train_smote = x_train\n",
    "    y_train_smote = y_train\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"x_train shape:\", x_train_smote.shape)\n",
    "print(\"y_train shape:\", y_train_smote.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create MobileNet model\n",
    "def create_mobilenet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # 2 output neurons for 2 categories\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Get the shape of the extracted features\n",
    "input_shape = train_features.shape[1:]\n",
    "\n",
    "model = create_mobilenet_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\inceptionv3_legit_0-1_compleltly_smote_model_900_classification_final_2.keras'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model.fit(\n",
    "    x_train_smote, y_train_smote,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=900,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "final_model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\inceptionv3_legit_0-1_compleltly_smote_model_900_classification_final_2.keras'\n",
    "model.save(final_model_save_path)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Predicted class labels (argmax gives the class with highest probability)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Display visual confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# Calculate F1 score, precision, and recall (sensitivity) for each class\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum() - (cm[class_label].sum() + cm[:, class_label].sum() - cm[class_label, class_label])\n",
    "    fp = cm[:, class_label].sum() - cm[class_label, class_label]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity = [specificity_score(y_test, y_pred, i) for i in range(len(classes))]\n",
    "\n",
    "# Print results for each class\n",
    "print(\"\\nMetrics for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"F1 Score: {f1[i]:.4f}\")\n",
    "    print(f\"Precision: {precision[i]:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall[i]:.4f}\")\n",
    "    print(f\"Specificity: {specificity[i]:.4f}\")\n",
    "\n",
    "# Calculate Cohen's Kappa coefficient\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"\\nCohen's Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the class of any random image by using voting mechanism\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input as preprocess_inception\n",
    "from keras.applications.densenet import DenseNet121, preprocess_input as preprocess_densenet\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input as preprocess_mobilenet\n",
    "\n",
    "# Load the saved models\n",
    "inceptionv3_model = load_model(r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Finalized Models\\inceptionv3_legit_compleltly_smote_model_900_classification_final_1.keras\")\n",
    "densenet_model = load_model(r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Finalized Models\\densenet_legit_compleltly_smote_model_900_classification_final_2.keras\")\n",
    "mobilenet_model = load_model(r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Finalized Models\\mobilenet_legit_compleltly_smote_model_900_classification_final_2.keras\")\n",
    "\n",
    "# Load the additional specialized models\n",
    "inceptionv3_01_model = load_model(r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Finalized Models\\inceptionv3_legit_0-1_compleltly_smote_model_900_classification_final_2.keras\")\n",
    "inceptionv3_23_model = load_model(r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Finalized Models\\inceptionv3_legit_2-3_compleltly_smote_model_900_classification_final_2.keras\")\n",
    "mobilenet_01_model = load_model(r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Finalized Models\\mobilenet_legit_0-1_compleltly_smote_model_900_classification_final_2.keras\")\n",
    "\n",
    "\n",
    "# Load base models for feature extraction\n",
    "inceptionv3_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "densenet_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "mobilenet_base = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "classes = ['Mayo 0', 'Mayo 1', 'Mayo 2', 'Mayo 3']\n",
    "\n",
    "def resize_features(features, target_size):\n",
    "    return tf.image.resize(features, target_size)\n",
    "\n",
    "def predict_with_model(model, base_model, img_path, target_size, preprocess_func, feature_target_size=None):\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_func(img_array)\n",
    "    \n",
    "    features = base_model.predict(img_array)\n",
    "    \n",
    "    if feature_target_size:\n",
    "        features = resize_features(features, feature_target_size)\n",
    "    \n",
    "    prediction = model.predict(features)\n",
    "    return prediction[0]\n",
    "\n",
    "def voting_prediction(img_path):\n",
    "    inceptionv3_pred = predict_with_model(inceptionv3_model, inceptionv3_base, img_path, (299, 299), preprocess_inception, (5, 5))\n",
    "    densenet_pred = predict_with_model(densenet_model, densenet_base, img_path, (224, 224), preprocess_densenet)\n",
    "    mobilenet_pred = predict_with_model(mobilenet_model, mobilenet_base, img_path, (224, 224), preprocess_mobilenet)\n",
    "    \n",
    "    # Predictions from specialized models\n",
    "    inceptionv3_01_pred = predict_with_model(inceptionv3_01_model, inceptionv3_base, img_path, (299, 299), preprocess_inception, (5, 5))\n",
    "    inceptionv3_23_pred = predict_with_model(inceptionv3_23_model, inceptionv3_base, img_path, (299, 299), preprocess_inception, (5, 5))\n",
    "    mobilenet_01_pred = predict_with_model(mobilenet_01_model, mobilenet_base, img_path, (224, 224), preprocess_mobilenet)\n",
    "    \n",
    "    \n",
    "    # Combine predictions\n",
    "    combined_pred = np.zeros(4)\n",
    "    \n",
    "    # For Mayo 0 and 1\n",
    "    combined_pred[:2] = (inceptionv3_pred[:2] + densenet_pred[:2] + mobilenet_pred[:2] + \n",
    "                         inceptionv3_01_pred + mobilenet_01_pred) / 5\n",
    "    \n",
    "    # For Mayo 2 and 3\n",
    "    combined_pred[2:] = (inceptionv3_pred[2:] + densenet_pred[2:] + mobilenet_pred[2:] + \n",
    "                         inceptionv3_23_pred) / 4  # Removed mobilenet_23_pred\n",
    "    \n",
    "    # Get the predicted label and confidence score\n",
    "    predicted_class = np.argmax(combined_pred)\n",
    "    predicted_label = classes[predicted_class]\n",
    "    confidence = combined_pred[predicted_class]\n",
    "    \n",
    "    return (inceptionv3_pred, densenet_pred, mobilenet_pred, \n",
    "            inceptionv3_01_pred, inceptionv3_23_pred, \n",
    "            mobilenet_01_pred, \n",
    "            combined_pred, predicted_label, confidence)\n",
    "\n",
    "# Example usage:\n",
    "image_path = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\Internship-Deep Learning\\Internet Images\\Mayo1internet.png\"\n",
    "(inceptionv3_pred, densenet_pred, mobilenet_pred, \n",
    " inceptionv3_01_pred, inceptionv3_23_pred, \n",
    " mobilenet_01_pred, \n",
    " combined_pred, predicted_label, confidence) = voting_prediction(image_path)\n",
    "\n",
    "# Print results\n",
    "print(f'Predicted Label: {predicted_label}')\n",
    "print(f'Confidence: {confidence}')\n",
    "\n",
    "# Print probabilities for each class\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name} probabilities:\")\n",
    "    print(f\"InceptionV3: {inceptionv3_pred[i]:.4f}\")\n",
    "    print(f\"DenseNet: {densenet_pred[i]:.4f}\")\n",
    "    print(f\"MobileNet: {mobilenet_pred[i]:.4f}\")\n",
    "    print(f\"InceptionV3 Mayo 0/1: {inceptionv3_01_pred[i%2]:.4f}\")\n",
    "    print(f\"InceptionV3 Mayo 2/3: {inceptionv3_23_pred[i%2]:.4f}\")\n",
    "    print(f\"MobileNet Mayo 0/1: {mobilenet_01_pred[i%2]:.4f}\")\n",
    "    print(f\"Combined: {combined_pred[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNet 0-1 classification\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths\n",
    "train_val_dir = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\0-1 classification\"\n",
    "\n",
    "# Data preparation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define classes\n",
    "classes = ['Mayo 0','Mayo 1']\n",
    "\n",
    "# Load data with correct class_mode='sparse'\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# Print class indices and expected number of batches\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "print(\"Expected number of batches:\", len(train_generator))\n",
    "\n",
    "# Load MobileNet model without the top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(generator, model, num_augmentations=5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_batches = num_samples // generator.batch_size + (1 if num_samples % generator.batch_size else 0)\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        generator.reset()\n",
    "        for _ in range(num_batches):\n",
    "            inputs_batch, labels_batch = next(generator)\n",
    "            features_batch = model.predict(inputs_batch)\n",
    "            features.append(features_batch)\n",
    "            labels.append(labels_batch)\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(train_generator, model)\n",
    "\n",
    "# Troubleshooting: Check unique classes and their distribution\n",
    "print(\"Unique classes:\", np.unique(train_labels))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initial train-test split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Reshape features for SMOTE (only for training set)\n",
    "n_samples, x, y, z = x_train.shape\n",
    "x_train_reshaped = x_train.reshape((n_samples, x * y * z))\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    # Reshape back to original feature shape\n",
    "    x_train_smote = x_train_smote.reshape((-1, x, y, z))\n",
    "else:\n",
    "    print(\"Skipping SMOTE due to insufficient classes\")\n",
    "    x_train_smote = x_train\n",
    "    y_train_smote = y_train\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"x_train shape:\", x_train_smote.shape)\n",
    "print(\"y_train shape:\", y_train_smote.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create MobileNet model\n",
    "def create_mobilenet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # 2 output neurons for 2 categories\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Get the shape of the extracted features\n",
    "input_shape = train_features.shape[1:]\n",
    "\n",
    "model = create_mobilenet_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\mobilenet_legit_0-1_compleltly_smote_model_900_classification_final_2.keras'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model.fit(\n",
    "    x_train_smote, y_train_smote,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=900,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "final_model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\mobilenet_legit_0-1_compleltly_smote_model_900_classification_final_2.keras'\n",
    "model.save(final_model_save_path)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Predicted class labels (argmax gives the class with highest probability)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Display visual confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# Calculate F1 score, precision, and recall (sensitivity) for each class\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum() - (cm[class_label].sum() + cm[:, class_label].sum() - cm[class_label, class_label])\n",
    "    fp = cm[:, class_label].sum() - cm[class_label, class_label]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity = [specificity_score(y_test, y_pred, i) for i in range(len(classes))]\n",
    "\n",
    "# Print results for each class\n",
    "print(\"\\nMetrics for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"F1 Score: {f1[i]:.4f}\")\n",
    "    print(f\"Precision: {precision[i]:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall[i]:.4f}\")\n",
    "    print(f\"Specificity: {specificity[i]:.4f}\")\n",
    "\n",
    "# Calculate Cohen's Kappa coefficient\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"\\nCohen's Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InceptionV3 2-3 classification\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths\n",
    "train_val_dir = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\2-3 classification\"\n",
    "\n",
    "# Data preparation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define classes\n",
    "classes = ['Mayo 2','Mayo 3']\n",
    "\n",
    "# Load data with correct class_mode='sparse'\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# Print class indices and expected number of batches\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "print(\"Expected number of batches:\", len(train_generator))\n",
    "\n",
    "# Load MobileNet model without the top layers\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(generator, model, num_augmentations=5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_batches = num_samples // generator.batch_size + (1 if num_samples % generator.batch_size else 0)\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        generator.reset()\n",
    "        for _ in range(num_batches):\n",
    "            inputs_batch, labels_batch = next(generator)\n",
    "            features_batch = model.predict(inputs_batch)\n",
    "            features.append(features_batch)\n",
    "            labels.append(labels_batch)\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(train_generator, model)\n",
    "\n",
    "# Troubleshooting: Check unique classes and their distribution\n",
    "print(\"Unique classes:\", np.unique(train_labels))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initial train-test split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Reshape features for SMOTE (only for training set)\n",
    "n_samples, x, y, z = x_train.shape\n",
    "x_train_reshaped = x_train.reshape((n_samples, x * y * z))\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    # Reshape back to original feature shape\n",
    "    x_train_smote = x_train_smote.reshape((-1, x, y, z))\n",
    "else:\n",
    "    print(\"Skipping SMOTE due to insufficient classes\")\n",
    "    x_train_smote = x_train\n",
    "    y_train_smote = y_train\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"x_train shape:\", x_train_smote.shape)\n",
    "print(\"y_train shape:\", y_train_smote.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create MobileNet model\n",
    "def create_inceptionv3_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # 2 output neurons for 2 categories\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Get the shape of the extracted features\n",
    "input_shape = train_features.shape[1:]\n",
    "\n",
    "model = create_inceptionv3_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\inceptionv3_legit_2-3_compleltly_smote_model_900_classification_final_2.keras'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model.fit(\n",
    "    x_train_smote, y_train_smote,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=900,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "final_model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\inceptionv3_legit_2-3_compleltly_smote_model_900_classification_final_2.keras'\n",
    "model.save(final_model_save_path)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Predicted class labels (argmax gives the class with highest probability)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Display visual confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# Calculate F1 score, precision, and recall (sensitivity) for each class\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum() - (cm[class_label].sum() + cm[:, class_label].sum() - cm[class_label, class_label])\n",
    "    fp = cm[:, class_label].sum() - cm[class_label, class_label]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity = [specificity_score(y_test, y_pred, i) for i in range(len(classes))]\n",
    "\n",
    "# Print results for each class\n",
    "print(\"\\nMetrics for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"F1 Score: {f1[i]:.4f}\")\n",
    "    print(f\"Precision: {precision[i]:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall[i]:.4f}\")\n",
    "    print(f\"Specificity: {specificity[i]:.4f}\")\n",
    "\n",
    "# Calculate Cohen's Kappa coefficient\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"\\nCohen's Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNet classification\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths\n",
    "train_val_dir = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\HyperKvasir Classification\"\n",
    "\n",
    "# Data preparation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define classes\n",
    "classes = ['Mayo 0','Mayo 1','Mayo 2','Mayo 3']\n",
    "\n",
    "# Load data with correct class_mode='sparse'\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# Print class indices and expected number of batches\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "print(\"Expected number of batches:\", len(train_generator))\n",
    "\n",
    "# Load MobileNet model without the top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(generator, model, num_augmentations=5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_batches = num_samples // generator.batch_size + (1 if num_samples % generator.batch_size else 0)\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        generator.reset()\n",
    "        for _ in range(num_batches):\n",
    "            inputs_batch, labels_batch = next(generator)\n",
    "            features_batch = model.predict(inputs_batch)\n",
    "            features.append(features_batch)\n",
    "            labels.append(labels_batch)\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(train_generator, model)\n",
    "\n",
    "# Troubleshooting: Check unique classes and their distribution\n",
    "print(\"Unique classes:\", np.unique(train_labels))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initial train-test split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Reshape features for SMOTE (only for training set)\n",
    "n_samples, x, y, z = x_train.shape\n",
    "x_train_reshaped = x_train.reshape((n_samples, x * y * z))\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    # Reshape back to original feature shape\n",
    "    x_train_smote = x_train_smote.reshape((-1, x, y, z))\n",
    "else:\n",
    "    print(\"Skipping SMOTE due to insufficient classes\")\n",
    "    x_train_smote = x_train\n",
    "    y_train_smote = y_train\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"x_train shape:\", x_train_smote.shape)\n",
    "print(\"y_train shape:\", y_train_smote.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create MobileNet model\n",
    "def create_mobilenet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(4, activation='softmax')(x)  # 4 output neurons for 4 categories\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Get the shape of the extracted features\n",
    "input_shape = train_features.shape[1:]\n",
    "\n",
    "model = create_mobilenet_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\mobilenet_legit_compleltly_smote_model_900_classification_final_2.keras'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model.fit(\n",
    "    x_train_smote, y_train_smote,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=900,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "final_model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\mobilenet_legit_compleltly_smote_model_900_classification_final_2.keras'\n",
    "model.save(final_model_save_path)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Predicted class labels (argmax gives the class with highest probability)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Display visual confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# Calculate F1 score, precision, and recall (sensitivity) for each class\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum() - (cm[class_label].sum() + cm[:, class_label].sum() - cm[class_label, class_label])\n",
    "    fp = cm[:, class_label].sum() - cm[class_label, class_label]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity = [specificity_score(y_test, y_pred, i) for i in range(len(classes))]\n",
    "\n",
    "# Print results for each class\n",
    "print(\"\\nMetrics for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"F1 Score: {f1[i]:.4f}\")\n",
    "    print(f\"Precision: {precision[i]:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall[i]:.4f}\")\n",
    "    print(f\"Specificity: {specificity[i]:.4f}\")\n",
    "\n",
    "# Calculate Cohen's Kappa coefficient\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"\\nCohen's Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InceptionV3 classification\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths\n",
    "train_val_dir = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\HyperKvasir Classification\"\n",
    "\n",
    "# Data preparation\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define classes\n",
    "classes = ['Mayo 0','Mayo 1','Mayo 2','Mayo 3']\n",
    "\n",
    "# Load data with correct class_mode='sparse'\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# Print class indices and expected number of batches\n",
    "print(\"Class indices:\", train_generator.class_indices)\n",
    "print(\"Expected number of batches:\", len(train_generator))\n",
    "\n",
    "# Load MobileNet model without the top layers\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(generator, model, num_augmentations=5):\n",
    "    features = []\n",
    "    labels = []\n",
    "    num_samples = len(generator.filenames)\n",
    "    num_batches = num_samples // generator.batch_size + (1 if num_samples % generator.batch_size else 0)\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        generator.reset()\n",
    "        for _ in range(num_batches):\n",
    "            inputs_batch, labels_batch = next(generator)\n",
    "            features_batch = model.predict(inputs_batch)\n",
    "            features.append(features_batch)\n",
    "            labels.append(labels_batch)\n",
    "\n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(train_generator, model)\n",
    "\n",
    "# Troubleshooting: Check unique classes and their distribution\n",
    "print(\"Unique classes:\", np.unique(train_labels))\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Initial train-test split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2222, random_state=42)\n",
    "\n",
    "# Reshape features for SMOTE (only for training set)\n",
    "n_samples, x, y, z = x_train.shape\n",
    "x_train_reshaped = x_train.reshape((n_samples, x * y * z))\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "if len(np.unique(y_train)) > 1:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    # Reshape back to original feature shape\n",
    "    x_train_smote = x_train_smote.reshape((-1, x, y, z))\n",
    "else:\n",
    "    print(\"Skipping SMOTE due to insufficient classes\")\n",
    "    x_train_smote = x_train\n",
    "    y_train_smote = y_train\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"x_train shape:\", x_train_smote.shape)\n",
    "print(\"y_train shape:\", y_train_smote.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create InceptionV3 model\n",
    "def create_inceptionv3_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(4, activation='softmax')(x)  # 4 output neurons for 4 categories\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Get the shape of the extracted features\n",
    "input_shape = train_features.shape[1:]\n",
    "\n",
    "model = create_inceptionv3_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\inceptionv3_legit_compleltly_smote_model_900_classification_final_1.keras'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Train the model with augmented data\n",
    "history = model.fit(\n",
    "    x_train_smote, y_train_smote,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=900,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "final_model_save_path = r'C:\\Users\\nanda\\OneDrive\\Desktop\\inceptionv3_legit_compleltly_smote_model_900_classification_final_1.keras'\n",
    "model.save(final_model_save_path)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Predicted class labels (argmax gives the class with highest probability)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Display visual confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# Calculate F1 score, precision, and recall (sensitivity) for each class\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, class_label):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum() - (cm[class_label].sum() + cm[:, class_label].sum() - cm[class_label, class_label])\n",
    "    fp = cm[:, class_label].sum() - cm[class_label, class_label]\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "specificity = [specificity_score(y_test, y_pred, i) for i in range(len(classes))]\n",
    "\n",
    "# Print results for each class\n",
    "print(\"\\nMetrics for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"F1 Score: {f1[i]:.4f}\")\n",
    "    print(f\"Precision: {precision[i]:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {recall[i]:.4f}\")\n",
    "    print(f\"Specificity: {specificity[i]:.4f}\")\n",
    "\n",
    "# Calculate Cohen's Kappa coefficient\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"\\nCohen's Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
